---
title: "Plots/Graphics in R and Introduction to Machine Learning"
subtitle: "Summer Boot Camp: Day 4, University of Essex"
author: "Haider Raza, PhD"
date: "Wednesday, September 16, 2021"
output:
  html_document:
    df_print: paged
  pdf_document: default
authors: Haider Raza
---

Approximate time: 120 minutes


## Basic plots in R
R has a number of built-in tools for basic graph types such as histograms, scatter plots, bar charts, boxplots and much [more](http://www.statmethods.net/graphs/). Rather than going through all of different types, we will focus on `plot()`, a generic function for plotting x-y data. 

To get a quick view of the different things you can do with `plot`, let's use the `example()` function:
```r	
example("plot")
```

Here, you will find yourself having to press `<Return>` so you can scroll through the different types of graphs generated by `plot`. Take note of the different parameters used with each command and how that affects the aesthetics of the plot. 

```r
dev.off() 
# this means "device off" and we will be going over what this does at the end of this section. 
# For now, it makes it so that when we draw plots they show up where they are supposed to?
```

### Line Charts

First we'll produce a very simple graph using the values in the `car` vector:

```r
# Define the cars vector with 5 values
cars <- c(1, 3, 6, 4, 9)

# Graph the cars vector with all defaults
plot(cars)
```
![](../img/r4_1.png)

Let's add a title, a line to connect the points, and some color:

```r
# Define the cars vector with 5 values
cars <- c(1, 3, 6, 4, 9)

# Graph cars using blue points overlayed by a line 
plot(cars, type="o", col="blue")

# Create a title with a red, bold/italic font
title(main="Autos", col.main="red", font.main=4)
```
![](../img/r4_2.png)

Now let's add a red line for trucks and specify the y-axis range directly so it will be large enough to fit the truck data:

```r
# Define 2 vectors
cars <- c(1, 3, 6, 4, 9)
trucks <- c(2, 5, 4, 5, 12)

# Graph cars using a y axis that ranges from 0 to 12
plot(cars, type="o", col="blue", ylim=c(0,12))

# Graph trucks with red dashed line and square points
lines(trucks, type="o", pch=22, lty=2, col="red")

# Create a title with a red, bold/italic font
title(main="Autos", col.main="red", font.main=4)
```
![](../img/r4_3.png)

### Bar Charts

Let's start with a simple bar chart graphing the cars vector:

```r
# Define the cars vector with 5 values
cars <- c(1, 3, 6, 4, 9)

# Graph cars
barplot(cars)
```
![](../img/r4_4.png)

### Histograms

Let's start with a simple histogram graphing the distribution of the suvs vector:

```r
# Define the suvs vector with 5 values
suvs <- c(4,4,6,6,16)

# Create a histogram for suvs
hist(suvs)
```
![](../img/r4_5.png)

### Pie Charts

Let's start with a simple pie chart graphing the cars vector:

```r
# Define cars vector with 5 values
cars <- c(1, 3, 6, 4, 9)

# Create a pie chart for cars
pie(cars)
```
![](../img/r4_6.png)

## Advance Plotting in R with ggplot2 

`ggplot2` is an R package used for statistical computing and data representation using data visualization. It follows underlying graphics called `Grammar of Graphics`, which includes certain rules and independent components which can be used to represent data in various formats. 


### Relationship between “Grammar of Graphics” and R

`ggplot` uses statistical graphics for mapping the data to aesthetic attributes such as color, shape, size of the concerned geometric objects like points, lines and bars. The plot may also contain various statistical transformations of the concerned data which is drawn on the mentioned coordinate system.

We can build different types of plots which can be created with reference to the grammar:

* Data
* Layers
* Scales
* Coordinate Systems
* Faceting
* Theme

To install the package, we need to run

```r
install.packages("ggplot2")
```

To load the particular package, we need to follow the below mentioned syntax −

```r
library(ggplot2)
```

Let's read our first data

```r
data(iris)
```

Plotting the iris dataset plot with ggplot2 in simpler manner involves the following syntax −

```r
iris <- ggplot(iris, aes(Sepal.Length, Petal.Length, colour=Species)) + geom_point()
print(iris)
```
![](../img/r4_7.png)

Here we can modify the title, x label and y label which means x axis and y axis labels in systematic format as given below −

```r
print(iris + labs(y="Petal length (cm)", x = "Sepal length (cm)") 
      + ggtitle("Petal and sepal length of iris"))
```
![](../img/r4_8.png)

### Highlight and tick marks

Plot the markers with mentioned co-ordinates of x and y axes as mentioned below. It includes adding text, repeating text, highlighting particular area and adding segment as follows −

```r
# add text
iris + annotate("text", x = 6, y = 5, label = "text")

# add repeat
iris + annotate("text", x = 4:6, y = 5:7, label = "text")

# highlight an area
iris + annotate("rect", xmin = 5, xmax = 7, ymin = 4, ymax = 6, alpha = .5)

# segment
iris + annotate("segment", x = 5, xend = 7, y = 4, yend = 5, colour = "black")
```

|                  |  | 
|:-----------------|----:|
|![](../img/r_4_9_1.png)| ![](../img/r_4_9_2.png)| 
|![](../img/r_4_9_3.png)| ![](../img/r_4_9_4.png)| 


### Now let's explore another dataset (i.e. PlantGrowth Dataset)

A simple plot with required x and y axis of the graph as mentioned below

```r
plant_plot <- ggplot(PlantGrowth, aes(x=group, y=weight)) + 
  geom_point()
print(plant_plot)
```
![](../img/r4_10.png)

Task 1: 
`Can you please swipe x and y axes?`


### Legends in `ggplot`

Let's load the `iris` data again and plot

```r
data(iris)
iris <- ggplot(iris, aes(Sepal.Length, Petal.Length, colour=Species)) + geom_point()
print(iris)
```
![](../img/r4_11.png)

Now let's remove the legend

```r
iris + theme(legend.position = "none")
```
![](../img/r4_12.png)


Now let's bring it back and change the legend position to top

```r
data(iris)
iris <- ggplot(iris, aes(Sepal.Length, Petal.Length, colour=Species)) + geom_point()
iris<- iris + theme(legend.position = "top")
print(iris)
```
![](../img/r4_13.png)

We can also change the font style of legends

```r
library(ggplot2)
data(iris)
iris <- ggplot(iris, aes(Sepal.Length, Petal.Length, colour=Species)) + geom_point()
print(iris)
iris <- iris + theme(legend.title = element_text(colour = "blue", size = 10))
print(iris)
iris <- iris + theme(legend.text = element_text(colour = "red", size = 10))
print(iris)
```
![](../img/r4_14.png)

### Scatter Plot in R

For creating a basic scatter plot following command is executed 
```r
library(ggplot2)
data(iris)
iris <- ggplot(iris, aes(Sepal.Length, Petal.Length)) + geom_point()
print(iris)
```
![](../img/r4_15.png)

We can change the shape of points with a property called shape in geom_point() function.

```r
library(ggplot2)
data(iris)
iris <- ggplot(iris, aes(Sepal.Length, Petal.Length)) + geom_point(shape = 1)
print(iris)
```
![](../img/r4_16.png)

We can add color to the points which is added in the required scatter plots.

```r
library(ggplot2)
data(iris)
iris <- ggplot(iris, aes(Sepal.Length, Petal.Length, colour  = Species)) + geom_point(shape = 1)
print(iris)
```
![](../img/r4_17.png)

### Smoothing and fitting regression

We will focus on establishing relationship between the variables.

`geom_smooth` function aids the pattern of overlapping and creating the pattern of required variables. The attribute method “lm” mentions the regression line which needs to be developed.

```r
library(ggplot2)
data(iris)
iris <- ggplot(iris, aes(Sepal.Length, Petal.Length, colour  = Species)) + 
  geom_point(shape = 1) +
  geom_smooth(method = lm)
print(iris)
```
![](../img/r4_18.png)

We can also add a regression line with no shaded confidence region with below mentioned:

```r
library(ggplot2)
data(iris)
iris <- ggplot(iris, aes(Sepal.Length, Petal.Length, colour  = Species)) + 
  geom_point(shape = 1) +
  geom_smooth(method = lm, se = FALSE)
print(iris)
```
![](../img/r4_19.png)

### Bar Plots & Histograms

Bar plots represent the categorical data in rectangular manner. The bars can be plotted vertically and horizontally. The heights or lengths are proportional to the values represented in graphs. The `x` and `y` axes of bar plots specify the category which is included in specific data set.

Histogram is a bar graph which represents the raw data with clear picture of distribution of mentioned data set.

Let's inspect the MPG dataset. This dataset contains a subset of the fuel economy data that the EPA makes available on https://fueleconomy.gov/. It contains only models which had a new release every year between 1999 and 2008 - this was used as a proxy for the popularity of the car.

```r
library(ggplot2)
mpg
```

Now let's create a first bar count plot. We will use geom_bar() function which is used for creating bar plots. It takes the attribute of statistical value called count.

```r
b_plot<-ggplot(mpg, aes(x=factor(cyl))) + geom_bar(stat = "count")
b_plot
```
![](../img/r_4_20.png)

Now let's create our first histogram. We will use geom_histogram(), which includes all the necessary attributes for creating a histogram. Here, it takes the attribute of hwy with respective count. The color is taken as per the requirements.


```r
ggplot(data = mpg, aes(x=hwy)) + 
  geom_histogram(col = "red",
                 fill = "green", 
                 alpha = 0.2, 
                 binwidth = 5)
```
![](../img/r_4_22.png)


### Box plot

The `ggplot2` library allows to make a boxplot using `geom_boxplot()`. You have to specify a quantitative variable for the Y axis, and a qualitative variable for the X axis ( a group).


```r
library(ggplot2)
 
# The mtcars dataset is natively available
# head(mtcars)
 
# A really basic boxplot.
ggplot(mtcars, aes(x=as.factor(cyl), y=mpg)) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("cyl")
```
![](../img/r4_23.png)



## Statistics in R

Statistical analysis in R is performed by using many in-built functions. Most of these functions are part of the R base package. These functions take R vector as an input along with the arguments and give the result.

We will be discussing mean, median and mode.

![](../img/meanmedian.png)


### Mean
It is calculated by taking the sum of the values and dividing with the number of values in a data series. The function `mean()` is used to calculate this in R.


```r
# Create a vector. 
x <- c(12,7,3,4.2,18,2,54,-21,8,-5)
# Find Mean.
result.mean <- mean(x)
print(result.mean)
```

#### Applying NA Option

If there are missing values, then the mean function returns NA.
To drop the missing values from the calculation use `na.rm = TRUE`. which means remove the `NA` values.

```r
x <- c(12,7,3,4.2,18,2,54,-21,8,-5,NA)

# Find mean
result.mean <-  mean(x)
print(result.mean)

# Find mean dropping NA values
result.mean <-  mean(x,na.rm = TRUE)
print(result.mean)
```

### Median
The middle most value in a data series is called the median. The `median()` function is used in R to calculate this value.

```r
# Create the vector.
x <- c(12,7,3,4.2,18,2,54,-21,8,-5)

# Find the median.
median.result <- median(x)
print(median.result)
```

### Mode
The mode is the value that has highest number of occurrences in a set of data. Unike mean and median, mode can have both numeric and character data. 

R does not have a standard in-built function to calculate mode. So we create a user function to calculate mode of a data set in R. This function takes the vector as input and gives the mode value as output.

```r
# Create the function.
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Create the vector with numbers.
v <- c(2,1,2,3,1,2,3,4,1,5,5,3,2,3)

# Calculate the mode using the user function.
result <- getmode(v)
print(result)
```

We can calculate the mode of characters as well:

```r
# Create the vector with characters.
charv <- c("I","Love","Essex","Pizza","at", "Essex")

# Calculate the mode using the user function.
result <- getmode(charv)
print(result)
```

### Min and Max in R

The R `max` function returns the maximum value of a vector or column.
Let's take a vector from the above example:

```r
v <- c(2,1,2,3,1,2,3,4,1,5,5,3,2,3)
max(v)
```

The R `min` function returns the minimum value of a vector or column.

```r
v <- c(2,1,2,3,1,2,3,4,1,5,5,3,2,3)
min(v)
```

But the problem comes when you have NA in the vector

```r
v <- c(2,1,2,3,1,2,3,4,1,NA,5,3,2,3)
max(v)   
max(v, na.rm = TRUE)    
```

We can see the same in `min` function
```r
v <- c(2,1,2,3,1,2,3,4,1,NA,5,3,2,3)
min(v)   
min(v, na.rm = TRUE)    
```

## Introduction to Machine Learning in R

Machine learning is a branch in computer science that studies the design of algorithms that can learn. Typical machine learning tasks are concept learning, function learning or “predictive modeling”, clustering and finding predictive patterns. These tasks are learned through available data that were observed through experiences or instructions, for example. Machine learning hopes that including the experience into its tasks will eventually improve the learning. The ultimate goal is to improve the learning in such a way that it becomes automatic, so that humans like ourselves don’t need to interfere any more.


<img src="../img/MLflow.png" width="600"/>

### Types of Machine Learning

**Supervised learning**: the learning algorithm is presented with labelled example inputs, where the labels indicate the desired output. SML itself is composed of classification, where the output is categorical, and regression, where the output is numerical.

**Unsupervised learning**: no labels are provided, and the learning algorithm focuses solely on detecting structure in unlabelled input data.

**Reinforcement learning**: the learning algorithm performs a task using feedback from operating in a real or synthetic environment.

<img src="../img/ML.png"  width="600"/>


## Simple Linear Regression in R

Linear regression is used to predict a quantitative outcome $y$ on the basis of one single predictor variable $x$. The goal is to build a mathematical model (or formula) that defines $y$ as a function of the $x$ variable.

Once, we built a statistically significant model, it’s possible to use it for predicting future outcome on the basis of new $x$ values.

Consider that, we want to evaluate the impact of advertising budgets of three medias (`youtube`, `facebook` and `newspaper`) on future sales. This example of problem can be modeled with linear regression.


#### Formula and basics
The mathematical formula of the linear regression can be written as $y = b0 + b1*x + e$, where:

* $b0$ and $b1$ are known as the regression beta coefficients or parameters:
* $b0$ is the intercept of the regression line; that is the predicted value when $x = 0$.
* $b1$ is the slope of the regression line.
* $e$ is the error term (also known as the residual errors), the part of $y$ that can be explained by the regression model


The figure below illustrates the linear regression model, where:

<img src="../img/lr.png"  width="400"/>

* the best-fit regression line is in blue
* the intercept ($b0$) and the slope ($b1$) are shown in green
* the error terms ($e$) are represented by vertical red lines


From the scatter plot above, it can be seen that not all the data points fall exactly on the fitted regression line. Some of the points are above the blue curve and some are below it; overall, the residual errors ($e$) have approximately mean zero.

The sum of the squares of the residual errors are called the `Residual Sum of Squares` or `RSS`.

The average variation of points around the fitted regression line is called the `Residual Standard Error (RSE)`. This is one the metrics used to evaluate the overall quality of the fitted regression model. The lower the `RSE`, the better it is. Since the mean error term is zero, the outcome variable y can be approximately estimated as follow:


\(y \sim b0 + b1*x\)

Mathematically, the beta coefficients ($b0$ and $b1$) are determined so that the `RSS` is as minimal as possible. This method of determining the beta coefficients is technically called `least squares regression` or `ordinary least squares (OLS)` regression.

Once, the beta coefficients are calculated, a `t-test` is performed to check whether or not these coefficients are significantly different from zero. A non-zero beta coefficients means that there is a significant relationship between the predictors ($x$) and the outcome variable ($y$).

##### Loading required R packages

* tidyverse for data manipulation and visualization
* ggpubr: creates easily a publication ready-plot


```r
install.packages("ggpubr")
install.packages("datarium")
library(tidyverse)
library(ggpubr)
```

We’ll use the marketing data set [datarium package]. It contains the impact of three advertising medias (youtube, facebook and newspaper) on sales. Data are the advertising budget in thousands of dollars along with the sales. The advertising experiment has been repeated 200 times with different budgets and the observed sales have been recorded.

Let's load our dataset

```r
# Load the package
data("marketing", package = "datarium")
head(marketing, 4)

  youtube facebook newspaper sales
1  276.12    45.36     83.04 26.52
2   53.40    47.16     54.12 12.48
3   20.64    55.08     83.16 11.16
4  181.80    49.56     70.20 22.20
```
We want to predict `future sales` on the basis of `advertising budget spent on youtube`.


##### Visualization
Create a scatter plot displaying the sales units versus youtube advertising budget.
Add a smoothed line

```r
ggplot(marketing, aes(x = youtube, y = sales)) +
  geom_point() +
  stat_smooth()
```
<img src="../img/lrp1.png"  width="500"/>

The graph above suggests a linearly increasing relationship between the `sales` and the `youtube` variables. This is a good thing, because, one important assumption of the `linear regression` is that the relationship between the outcome and predictor variables is linear and additive.

It’s also possible to compute the correlation coefficient between the two variables using the R function `cor()`:

```r
cor(marketing$sales, marketing$youtube)
```

Output:

```r
Correlation: 0.78
```

The correlation coefficient measures the level of the association between two variables `x` and `y`. Its value ranges between `-1 (perfect negative correlation: when x increases, y decreases)` and `+1 (perfect positive correlation: when x increases, y increases)`.

A value closer to `0` suggests a weak relationship between the variables. A low correlation (-0.2 < x < 0.2) probably suggests that much of variation of the outcome variable (y) is not explained by the predictor (x). In such case, we should probably look for better predictor variables.

In our example, the correlation coefficient is large enough, so we can continue by building a linear model of y as a function of x.

##### Computation

The simple linear regression tries to find the best line to predict sales on the basis of youtube advertising budget. The linear model equation can be written as follow: `sales = b0 + b1 * youtube`

The R function `lm()` can be used to determine the beta coefficients of the linear model:

```r
model <- lm(sales ~ youtube, data = marketing)
model
```
Output:

```r
Call:
lm(formula = sales ~ youtube, data = marketing)

Coefficients:
(Intercept)      youtube  
    8.43911      0.04754 
```

##### Interpretation

From the output above:

the estimated regression line equation can be written as follow: `sales = 8.44 + 0.048*youtube`

the intercept `(b0) is 8.44`. It can be interpreted as the predicted sales unit for a zero youtube advertising budget. Recall that, we are operating in units of thousand dollars. This means that, for a youtube advertising budget equal zero, we can expect a sale of `8.44 *1000 = 8440` dollars.

the regression beta coefficient for the variable youtube `(b1)`, also known as the slope, is `0.048`. This means that, for a youtube advertising budget equal to `1000` dollars, we can expect an increase of `48` units `(0.048*1000)` in sales. That is, `sales = 8.44 + 0.048*1000 = 56.44` units. As we are operating in units of thousand dollars, this represents a sale of `56440` dollars.

##### Regression Line

To add the regression line onto the scatter plot, you can use the function `stat_smooth()` [ggplot2]. By default, the fitted line is presented with confidence interval around it. The confidence bands reflect the uncertainty about the line. If you don’t want to display it, specify the option `se = FALSE` in the function `stat_smooth()`.

```r
ggplot(marketing, aes(youtube, sales)) +
  geom_point() +
  stat_smooth(method = lm)
```
<img src="../img/lrp2.png"  width="500"/>

##### Model assessment
In the previous section, we built a linear model of sales as a function of youtube advertising budget: `sales = 8.44 + 0.048*youtube`.

Before using this formula to predict `future sales`, you should make sure that this model is statistically significant, that is:

* There is a statistically significant relationship between the predictor and the outcome variables
* the model that we built fits very well the data in our hand. 

Here we will check the quality of a linear regression model.

```r
summary(model)
```
Output:

```r
Call:
lm(formula = sales ~ youtube, data = marketing)

Residuals:
     Min       1Q   Median       3Q      Max 
-10.0632  -2.3454  -0.2295   2.4805   8.6548 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 8.439112   0.549412   15.36   <2e-16 ***
youtube     0.047537   0.002691   17.67   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 3.91 on 198 degrees of freedom
Multiple R-squared:  0.6119,	Adjusted R-squared:  0.6099 
F-statistic: 312.1 on 1 and 198 DF,  p-value: < 2.2e-16
```

The summary outputs shows 6 components, including:

* **Call**: Shows the function call used to compute the regression model.
* **Residuals**: Provide a quick view of the distribution of the residuals, which by definition have a mean zero. Therefore, the median should not be far from zero, and the minimum and maximum should be roughly equal in absolute value.
* **Coefficients**: Shows the regression beta coefficients and their statistical significance. Predictor variables, that are significantly associated to the outcome variable, are marked by stars.
* **Residual standard error (RSE)**, **R-squared (R2)** and the **F-statistic** are metrics that are used to check how well the model fits to our data.


### Let's start a demo of Machine Leanring

#### Step 1: Get Your Data
Machine learning usually starts from observed data. You can take your own data set or browse through other sources to find one. There are a few Built-in Datasets of R and we can use them.
In this example, we will be using the Iris data set, which is very well-known in the area of machine learning. This dataset is built into R, so you can take a look at this dataset by typing the following into your console:

<img src="../img/iris.png"  width="600"/>

Let's download data directly from UCI Machine Learning repository

```r
iris <- read.csv(url("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"), 
                 header = FALSE) 

# Print first lines
head(iris)

# Add column names
names(iris) <- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width", "Species")

# Check the result
iris
```

#### Step 2: Know Your Data

You need to get your hands dirty, explore and visualize your data set and even gather some more domain knowledge if you feel the data is way over your head.

```r
iris_p1 <- ggplot(iris, aes(Sepal.Length, Sepal.Width, colour=Species)) + geom_point()
print(iris_p1)
```

<img src="../img/irisdata.png"  width="600"/>

We can see that there is a high correlation between the `sepal length` and the `sepal width` of the Setosa iris flowers, while the correlation is somewhat less high for the Virginica and Versicolor flowers: the data points are more spread out over the graph and don’t form a cluster like you can see in the case of the Setosa flowers.

The scatter plot that maps the petal length and the petal width tells a similar story:

```r
iris_p2 <- ggplot(iris, aes(Petal.Length, Petal.Width, colour=Species)) + geom_point()
print(iris_p2)
```
<img src="../img/irisp2.png"  width="600"/>


We can see that above graph indicates a positive correlation between the petal length and the petal width for all different species that are included into the Iris data set. Of course, you probably need to test this hypothesis a bit further if you want to be really sure of this:

```r

# Overall correlation `Petal.Length` and `Petal.Width`
cor(iris$Petal.Length, iris$Petal.Width)

# Return values of `iris` levels 
x=levels(iris$Species)

# Print Setosa correlation matrix
print(x[1])
cor(iris[iris$Species==x[1],1:4])

# Print Versicolor correlation matrix
print(x[2])             
cor(iris[iris$Species==x[2],1:4])

# Print Virginica correlation matrix
print(x[3])
cor(iris[iris$Species==x[3],1:4])

```

We can see that when we combined all three species, the correlation was a bit stronger than it is when you look at the different species separately: the overall correlation is 0.96, while for Versicolor this is 0.79. Setosa and Virginica, on the other hand, have correlations of petal length and width at 0.31 and 0.32 when you round up the numbers.


#### Step 3: Create a Validation Dataset (Partitioning data)

We need to know that the model we created is any good.Later, we will use statistical methods to estimate the accuracy of the models that we create on unseen data. We also want a more concrete estimate of the accuracy of the best model on unseen data by evaluating it on actual unseen data.
That is, we are going to hold back some data that the algorithms will not get to see and we will use this data to get a second and independent idea of how accurate the best model might actually be.

We will split the loaded dataset into two, `80%` of which we will use to train our models and `20%` that we will hold back as a validation dataset.

To do machine learning, first we need to install a basic library 

```r
install.packages("caret")
library("caret")
```

Now let's partition the data

```r
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(iris$Species, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- iris[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- iris[validation_index,]

```

We have now training data in the dataset variable and a validation set we will use later in the validation variable. Note that we replaced our dataset variable with the `80%` sample of the dataset. This was an attempt to keep the rest of the code simpler and readable.

#### Univariate Plots

```r
x <- dataset[,1:4]
y <- dataset[,5]
```

```r

# boxplot for each attribute on one image
par(mfrow=c(1,4))
  for(i in 1:4) {
  boxplot(x[,i], main=names(iris)[i])}
```

<img src="../img/BoxIris.png"  width="600"/>

#### Step 4: Train and Evaluate Model

Now it is time to create some models of the data and estimate their accuracy on unseen (test) data.

Here is what we are going to cover in this step:

Set-up the test harness to use 10-fold cross validation.
Build 5 different models to predict species from flower measurements
Select the best model.


##### 10-fold crossvalidation
We will 10-fold cross validation to estimate accuracy. This will split our dataset into `10` parts, `train in 9` and `test on 1` and release for all combinations of train-test splits. We will also repeat the process 3 times for each algorithm with different splits of the data into 10 groups, in an effort to get a more accurate estimate.

```r
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
```
We are using the metric of “Accuracy” to evaluate models. This is a ratio of the number of correctly predicted instances in divided by the total number of instances in the dataset multiplied by 100 to give a percentage (e.g. 95% accurate). We will be using the metric variable when we run build and evaluate each model next.

##### Build Models (Training)

We don’t know which algorithms would be good on this problem or what configurations to use. We get an idea from the plots that some of the classes are partially linearly separable in some dimensions, so we are expecting generally good results.

Let’s evaluate 5 different algorithms:

* Linear Discriminant Analysis (LDA)
* Classification and Regression Trees (CART).
* k-Nearest Neighbors (kNN).
* Support Vector Machines (SVM) with a linear kernel.
* Random Forest (RF)

This is a good mixture of simple linear (LDA), nonlinear (CART, kNN) and complex nonlinear methods (SVM, RF). We reset the random number seed before reach run to ensure that the evaluation of each algorithm is performed using exactly the same data splits. It ensures the results are directly comparable.

```r

install.packages("e1071", dep = TRUE) 

# a) linear algorithms
set.seed(7)
fit.lda <- train(Species~., data=dataset, method="lda", metric=metric, trControl=control)
# b) nonlinear algorithms
# CART
set.seed(7)
fit.cart <- train(Species~., data=dataset, method="rpart", metric=metric, trControl=control)
# kNN
set.seed(7)
fit.knn <- train(Species~., data=dataset, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# SVM
set.seed(7)
fit.svm <- train(Species~., data=dataset, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(Species~., data=dataset, method="rf", metric=metric, trControl=control)

```

##### Now it is the time to select the best model

We now have 5 models and accuracy estimations for each. We need to compare the models to each other and select the most accurate. We can report on the accuracy of each model by first creating a list of the created models and using the summary function.

```r
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
```
Output:
```r
Call:
summary.resamples(object = results)

Models: lda, cart, knn, svm, rf 
Number of resamples: 10 

Accuracy 
          Min.   1st Qu. Median      Mean 3rd Qu. Max. NA's
lda  0.9166667 1.0000000      1 0.9833333       1    1    0
cart 0.8333333 0.8541667      1 0.9416667       1    1    0
knn  0.9166667 1.0000000      1 0.9833333       1    1    0
svm  0.8333333 0.9375000      1 0.9666667       1    1    0
rf   0.8333333 0.9166667      1 0.9583333       1    1    0

Kappa 
      Min. 1st Qu. Median   Mean 3rd Qu. Max. NA's
lda  0.875 1.00000      1 0.9750       1    1    0
cart 0.750 0.78125      1 0.9125       1    1    0
knn  0.875 1.00000      1 0.9750       1    1    0
svm  0.750 0.90625      1 0.9500       1    1    0
rf   0.750 0.87500      1 0.9375       1    1    0
```


We can also create a plot of the model evaluation results and compare the spread and the mean accuracy of each model. There is a population of accuracy measures for each algorithm because each algorithm was evaluated 10 times (10 fold cross validation).

```r
# compare accuracy of models
dotplot(results)
```
We can see that the most accurate model in this case was LDA:
<img src="../img/dotplot.png"  width="600"/>


The results for just the LDA model can be summarized.

```r
print(fit.lda)
```

Output:

```r
Linear Discriminant Analysis 

120 samples
  4 predictor
  3 classes: 'Iris-setosa', 'Iris-versicolor', 'Iris-virginica' 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 108, 108, 108, 108, 108, 108, ... 
Resampling results:

  Accuracy   Kappa
  0.9833333  0.975
```

#### Evaluate Model

The LDA was the most accurate model. Now we want to get an idea of the accuracy of the model on our validation set.

This will give us an independent final check on the accuracy of the best model. It is valuable to keep a validation set just in case you made a slip during such as overfitting to the training set or a data leak. Both will result in an overly optimistic result.

We can run the LDA model directly on the validation set and summarize the results in a confusion matrix.

```r
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.lda, validation)
confusionMatrix(predictions, validation$Species)
```

Output:

```r
Confusion Matrix and Statistics

                 Reference
Prediction        Iris-setosa Iris-versicolor Iris-virginica
  Iris-setosa              10               0              0
  Iris-versicolor           0               9              0
  Iris-virginica            0               1             10

Overall Statistics
                                          
               Accuracy : 0.9667          
                 95% CI : (0.8278, 0.9992)
    No Information Rate : 0.3333          
    P-Value [Acc > NIR] : 2.963e-13       
                                          
                  Kappa : 0.95            
                                          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: Iris-setosa Class: Iris-versicolor Class: Iris-virginica
Sensitivity                      1.0000                 0.9000                1.0000
Specificity                      1.0000                 1.0000                0.9500
Pos Pred Value                   1.0000                 1.0000                0.9091
Neg Pred Value                   1.0000                 0.9524                1.0000
Prevalence                       0.3333                 0.3333                0.3333
Detection Rate                   0.3333                 0.3000                0.3333
Detection Prevalence             0.3333                 0.3000                0.3667
Balanced Accuracy                1.0000                 0.9500                0.9750
```
